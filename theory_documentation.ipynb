{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Recently we have been witnesses of the massive hype around cryptocurrencies, used as an investment asset rather than a currency as well. Considering that from 2013 until now, there has been a bias regarding the trends in their price and real value; for instance, if we compare the price of the most famous cryptocurrency nowadays on the market (and the first one), Bitcoin, in different long-time intervals, an enormous difference is perceived between them: While in May 2013 its price never surpassed the $ 150 (US currency) threshold, in December 2017 its price reached its peak with $ 19.535,70.\n",
    "\n",
    "Such kind of facts, generate a significant misunderstanding about the risks and volatility implied in dealing with these markets where the less financial educated population are the most vulnerable. \n",
    "\n",
    "Precedents in Latin America countries, including Brazil and Colombia, could be pointed out using examples like when Forex market was widely introduced and began to operate in these countries. Several intermediate fraud schemas rised (including pyramidal schemas) intending to collect population money using speeches based on biased facts such as the hyperinflation of bitcoin price mentioned above. The success of this schemas was caused mainly because of the effectiveness of their speeches empowered by the lack of financial education of the targeted population.\n",
    "\n",
    "Consequently, similar fraud schemas based on cryptocurrencies speculation investment born in this case due either the lack of knowledge in finance and/or cryptocurrencies concept and the gaps existing due the absence of policies and laws to regulate such markets regardless the attempts encouraged by countries like China, Japan, Sweden and others, of framing out an efficient regulatory policy in this matter. \n",
    "\n",
    "Furthermore, it is clear that even for more educated population, is still hard to evaluate and have a broader sight about the real potential risks and gains when investing in these currencies in certain markets such as the Brazilian one. Consequently, this work aims to overcome the problems pointed above by delivering clearer evidences regarding the real behavior of these assets in diversified portfolios using â€œEfficient frontier Markowitz modelâ€, evolutionary-based algorithms and VaR metric (Value at Risk).\n",
    "\n",
    "\n",
    "The history regarding financial markets stands over several failures which led to important advances in the matter for the sake of failure prevention and smarter decisions. In 1987 for instance, soon after the stock market crash, the lack of means and tools for measuring the potential risk of an asset under certain market conditions led to the development of Value at Risk (VaR) and since then has seen widespread use across financial services organizations until it was standardized in 1999 by the Basel agreement (eafit) [spark analytics].\n",
    "\n",
    "\n",
    "(Still workin progress .. Even in earlier stages, theory regarding )\n",
    "\n",
    "However regardless such advances, erroneous behaviors still persist and it was the case of 2008â€™s crisis where the excess of confidence across the investors and regulatory intuitions mainly, leveraged by the constant growth and great returns the market offered for decades led to the bigger financial crisis in the modern time[eafit].\n",
    "\n",
    "Along with this happening but independently, advances in technology with the capability of defining the economic scenario in the years to come were being developed. That is the case of Blockchain, the technology behind the most famous cryptocurriencies, where the later ones (Bitcoin specifically) were the first succeed implemented applications of Blockchain in 2008. \n",
    "\n",
    "\n",
    "\n",
    "It is important to remark this work deals with cryptocurrencies as financial assets only. Technical details regarding how the technology behind these assets works, is out of the scope.\n",
    "\n",
    "After these events, with new players in the economy and considering the non-stable scenario fed by the past crisis, turned the market more volatile and hard to predict, thus surviving such contexts requires more sophisticated tools and strategies to manage investments more â€œcorrectlyâ€. \n",
    "\n",
    "From traditional invesemtent models: \n",
    "\n",
    "\n",
    "According to (EAFIT reference), the most known investement startegies are:\n",
    "\n",
    "** The equal wiehgt strategy:** Is one of the most used strategy in portfolio diversification due its simplicity in implementation and understading. It simply weights every stock in a portfolio equally.\n",
    "\n",
    "** Minimum variance strategy:** Based on Markoweitz portfolio optimizatoin model, from the optimizied portfolios set, the portafolio with the minimum value of variance is selected.\n",
    "\n",
    "** Mean variance straegy:** Once the Markowitz efficient frontier is calculated, Ratio of Sharpe will lead to significant clues about what portfolio may be the best one when the investor does not have a clear idea about the level of risk he would tolerate.\n",
    "\n",
    "As sawn since the beginning of this work, the model presented in this work includes the last two.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Related Concepts intriduction\n",
    "\n",
    "## 2.1. Modern portafolio theory introduction\n",
    "\n",
    "Modern portfolio theory (MPT) or mean-variance analysis is a matematical framework for assembling a portafolio composed by several assets looking for maximizing the *expected return* for a given level of risk. The key insight regarding this framework is that an asset risk and return, should be assesed by how it contributes to a portafolio's overall risk and return, rather that by itself [1 - ref. wikipedia]. \n",
    "\n",
    "It is an extension of diversification in investing, where is assumed that owning different type of assets is less risky than owining just one type. Harry Markowitz introduced MPT in 1952, and such work led him to be awarded a with Nobel price in economics[2 - wikipedia].\n",
    "\n",
    "\n",
    "### 2.1.1. Risk and expected return:\n",
    "\n",
    "* **Portfolio Return: ** Is the proportion-weighted combination of the constitutent asset's return.\n",
    "\n",
    "* **Portfolio volatility: ** Is a function of the correlation $\\rho_ij$ of the component assets, for all asset pairs *(i,j)*.\n",
    "\n",
    "###  2.1.2. Mathematical model: \n",
    "\n",
    "**Expected return**\n",
    "\n",
    "$$E(R_p)=\\sum_{i}w_iE(R_i)$$\n",
    "\n",
    "Where $R_p$ is the return of the portfolio, $R_i$ is the return on asset $i$ ans $w_i$ is the weighting of component asset (or the proportion of asset $i$ in the portfolio)\n",
    "\n",
    "    \n",
    "**Portfolio return variance**\n",
    "\n",
    "$$\\sigma_p^2 = \\sum_{i}w_i^2\\sigma_i^2 + \\sum_{i}\\sum_{j}w_iw_j\\sigma_i\\sigma_j\\rho_{ij}$$\n",
    "\n",
    "\n",
    "Where $\\sigma$ is the standard deviation (sample) of the periodic returns on an asset, and $\\rho_{ij}$ is the correlation coefficient between the returns on assets i and j. Alternatively the expression can be written as:\n",
    "\n",
    "$$\\sigma_p^2 = \\sum_{i}\\sum_{j}w_iw_j\\sigma_i\\sigma_j\\rho_{ij}$$\n",
    "\n",
    "Where $\\rho_{ij} = 1$ for $i=j$, or\n",
    "\n",
    "$$\\sigma_p^2 = \\sum_{i}\\sum_{j}w_iw_j\\sigma_{ij}$$\n",
    "\n",
    "\n",
    "where $\\sigma_{ij} = \\sigma_i\\sigma_j\\rho_{ij}$ is the (sample) covariance of the periodic returns on the two assets, or alternatively denoted as $\\sigma(i,j), cov_{ij}$ or $cov(i,j)$.\n",
    "\n",
    "**Portfolio return volatility (standard deviation)**\n",
    "\n",
    "$$\\sigma_p = \\sqrt{\\sigma_p^2}$$\n",
    "\n",
    "\n",
    "The mathematical model definition of the portfolio volatility relies on an algebraic representation, expensive to calculate when the number of assets within the portfolio is big. In perspective, the expression tends to increment its size at a $2x + 1$ rate per asset:\n",
    "\n",
    "* 2 assets:\n",
    "\n",
    "$$\\sigma_p^2 = w_A^2\\sigma_A^2 + w_B^2\\sigma_B^2 + 2w_Aw_B\\sigma_A\\sigma_B\\rho_{AB}$$\n",
    "\n",
    "* 3 assets:\n",
    "\n",
    "$$\\sigma_p^2 = w_A^2\\sigma_A^2 + w_B^2\\sigma_B^2 + w_C^2\\sigma_C^2 + 2w_Aw_B\\sigma_A\\sigma_B\\rho_{AB} + 2w_Aw_C\\sigma_A\\sigma_C\\rho_{AC} + 2w_Bw_C\\sigma_B\\sigma_C\\rho_{BC}$$\n",
    "\n",
    "\n",
    "Then one of the main contributions delivered by Markowitz with his work, is the representation of the overall standard deviation or risk  in a matricial form which are computationally much cheaper to calculate.(expressions below).\n",
    "\n",
    "$$\n",
    "E(R_p) = \\left(\\begin{array}{cc} \n",
    "w_1 &  w_2 & ... & w_n\n",
    "\\end{array}\\right).\n",
    "\\left(\\begin{array}{cc} \n",
    "E(R_1)\\\\ \n",
    "E(R_2)\\\\\n",
    "...\\\\\n",
    "E(R_n)\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_p^2 = \\left(\\begin{array}{cc} \n",
    "w_1 &  w_2 & ... & w_n\n",
    "\\end{array}\\right).\n",
    "\\left(\\begin{array}{cc} \n",
    "\\sigma^2(A_{11}) & \\sigma^2(A_{12})  & ... & \\sigma^2(A_{1n}) \\\\\n",
    "\\sigma^2(A_{21}) & \\sigma^2(A_{22})  & ... & \\sigma^2(A_{2n}) \\\\\n",
    "... & ...  & ... & ... \\\\\n",
    "\\sigma^2(A_{m1}) & \\sigma^2(A_{m2})  & ... & \\sigma^2(A_{mn}) \\\\\n",
    "\\end{array}\\right).\n",
    "\\left(\\begin{array}{cc} \n",
    "w_1\\\\ \n",
    "w_2\\\\\n",
    "...\\\\\n",
    "w_n\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "....\n",
    "\n",
    "Despite the theorical importance of this model, criticisms in considering this is as a suitable tool for real-world investments have been pointed out in many ways because of the mismatching in modeling accurately the real world features.\n",
    "\n",
    "Fundamentally, two issues are worth to remark, firstly the inhability of MPT in considering the key parameters from the data that might explain the root causes that yield to a certain effect or behaviour, and secondly, as either the risk and correlation are based on *expected values*, very often they fail to take acocunt of new circumstances that did not exist when the historical data were generated.\n",
    "\n",
    "Nevertheless, for the purpose of this work, MTP is a enough for measuring and comprehend the real behaviour of the crypto in portfolios during the last months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Markowitz efficient frontier\n",
    "\n",
    "\n",
    "The efficient frontier (or portfolio frontier) is an investment portfolio which occupies the 'efficient' parts of the risk-return spectrum. Formally, it is the set of portfolios satisfying the condition that no other portfolio exists with a higher expected return but with the same risk of return...\n",
    "\n",
    "![Markowitz frontier illustration](./resources/img/markowitz_frontier.png \"Markowitz frontier illustration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Ratio of Sharpe.\n",
    "\n",
    "Once the Markowitz frontier is calculated, an investor is capable to pick the risk level he will tolerate considering what the frontier shape suggests: *The higher the expected return the portfolio might offer, the higher the risk the subject will embrace*. Yet in many cases, if not in the most, evaluate at which level of risk is desired to operate, could result in a difficult task to do. Therefore, the ratio of sharpe is introduced as an helpful criteria to aim an investor in such a decision.  \n",
    "\n",
    "For its calculation it is involved a free-risk asset in order to measure the *opportunity cost* an investor might miss, similarly to metrics such as *ROI* in finance or in marketing. \n",
    "\n",
    "********\n",
    "Therefore, this metric seeks for reaching out the best suited portfolio *complementar*\n",
    "********\n",
    "\n",
    "$$ Ratio \\ of \\ Sharpe = \\frac{ð‘Ÿ_ð‘ âˆ’ ð‘Ÿ_ð‘“}{\\sigma} $$\n",
    "\n",
    "Where $\\sigma = \\sqrt{Variance(ð‘Ÿ_ð‘ âˆ’ ð‘Ÿ_ð‘“)}$ ; if $ð‘Ÿ_ð‘“ $ is risk-free, then: $\\sigma = \\sigma_p$\n",
    "\n",
    "Finally \n",
    "\n",
    "$$ Ratio \\ of \\ Sharpe = \\frac{ð‘Ÿ_ð‘ âˆ’ ð‘Ÿ_ð‘“}{\\sigma_p} $$\n",
    "\n",
    "\n",
    "Further work could be leverage towards testing a modified Ratio of Sharpe considering VaR instead of the portfolio variance. This remains out of the scope of this work due it was demostrated that using either the normal *Ratio of Shape* or *the modified Ratio of Shape*, does not imply any significant gain in dealing with deveolping markets such as the Colombian one where the volatility is higher. This scenario is different in already developed markets such as the american where using VaR instead of variance does delivers siginificant improvements in precision; therefore testing how this modified version of such ratio works in the Brazilian market, is left as a future work.\n",
    "\n",
    "For the sake of simplicity it is assumed that Brazilian market maintains the same or even a higher level of volatily than the Colombian market.\n",
    "\n",
    "\n",
    "$$ Modified \\ Ratio \\ of \\ Sharpe = \\frac{ð‘Ÿ_ð‘ âˆ’ ð‘Ÿ_ð‘“}{ð‘‰ð‘Žð‘…} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4. Return to return rate\n",
    "\n",
    "Assets returns\n",
    "According to P. Morettin, the relative variance or the simple liquidity return $R_t$ of an asset is given by:\n",
    "\n",
    "$$Rt = \\frac{P_t - P_{t-1}}{P_{t-1}}$$\n",
    "As $R_t$ is defined as the expected return of an asset, generally expressed as a percent value, $r_t$ however is defined as the return rate of an asset, given by:\n",
    "\n",
    "$$r_t = ln(\\frac{P_t}{P_{t-1}})$$\n",
    "Therefore, a simple return $R_t$ is obtained by:\n",
    "\n",
    "$$ R_t = {e^{r_t}} - 1 $$\n",
    "The first term presented above $R_t$, is pretended to be used when the optimization takes place, while $r_t$ is the termo to be predicted with a GARCH model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. GARCH models - Getting return value of an asset by using \n",
    "\n",
    "[moretin]\n",
    "\n",
    "The efficient market theory indicates the prices of the returns incorporate the information available to all the stakeholders in the market, implyig that the variations for the prices are not predicatble (Fama, 1970; Campbell et. al, 1997). There are 3 categories of *efficency* depdending of the available information to the stakeholders:\n",
    "\n",
    "* **Weak efficiency:** Returns prices can not being predicted from past return prices. \n",
    "* **Semi-strong efficiency:** The basic set of data includes the history track of the return prices plus all the available public data. \n",
    "* **Strong efficiency:** The set of data includes all the public and private information. \n",
    "\n",
    "The \"weak efficiency\" scenario is very often to occur in stock prices prediction, hence by modeling the variance as a phenomena depending of time rather than the return price itself, is a valid approach. In order to model these more complex time series, models such as the called *stochastic volatility models*  are used. \n",
    "\n",
    "One of the most recalled type of these models is *ARCH* or *autoregressive conditional heteroscedasticity*, introduced by Engel in 1982 which aimed towards estimating the variance of the inflation phenomena in economics. This model stands by the over the fact a return $r_t$ is not serially correlated, but its conditional variance depends on past returns following a cuadratic function.\n",
    "\n",
    "$$ r_t = \\sqrt{h_t}\\epsilon_t$$\n",
    "$$ h_t = \\alpha_0 + \\alpha_1r_{t-1}^2 + ... + \\alpha_mr_{t-m}^2 $$\n",
    "\n",
    "Where $\\epsilon_t \\ i.i.d$ with $\\mu = 0, \\ \\alpha_0 > 0, \\alpha_i >, i = 1,..., m - 1, \\alpha_m > 0$\n",
    "\n",
    "A generalization of this model was proposed by Bollesrlev (1986, 1987, 1988) called *GARCH* or *Generalized ARCH*. This model aims to also describe the variance yet it stands in the idea that the same variance might be modeled by fewer parameters than ARCH models, similar of what is achieved by using ARMA models instead of  a pure single AR or MA model.\n",
    "\n",
    "A GARCH(p, q) could be described as an ARMA model applied to the variance of a time series, it has an autoregressive term and a moving average term. The AR(p) models the variance of the residuals (squared errors) or simply the time series squared. The MA(q) portion models the variance of the process.\n",
    "\n",
    "$$ r_t = \\sqrt{h_t}\\epsilon_t$$\n",
    "$$ h_t = \\alpha_0 + \\sum_{i=1}^{m}{\\alpha_ir_{t-i}^2} + \\sum_{j=1}^{n}{\\beta_jh_{t-j}} $$\n",
    "\n",
    "Where $ \\epsilon_t \\ i.i.d $ with $  \\mu = 0, \\ \\alpha_0 > 0, \\alpha_i >, i = 1,..., m - 1, \\ \\beta_j >= 0, \\ j = 1,...,n - 1, \\ \\alpha_m > 0, \\ \\beta_n > 0, \\ \\sum_{i=1}^{q}{(\\alpha_i+\\beta_j)} < 1, \\ q = max(m,n) $\n",
    "\n",
    "$\\epsilon_t$ distribution follows either a normal or t-student distribution.\n",
    "\n",
    "Hence a GARCH(1, 1) would look like the expression (23) below.\n",
    "\n",
    "$$ e_t = \\sigma_tw_t $$\n",
    "$$ \\sigma_t^2 =  \\alpha_0 + \\alpha_1e_{t-1}^2 + \\beta_1\\sigma_{t-1}^2$$\n",
    "\n",
    "Fitting a GARCH model to a time seris, is similar of fitting a time series using the ARIMA framework. Basically it is an optimization problem where is look to minimize a choosen parameter (usually AIC or BIC [look for refrence]) by testing several combinatins of parameters *p (autoregressive term), o (derivative order), q (moving average term)*. \n",
    "\n",
    "Other parameters for evaluating the fitness of a model relies on measuring the autocorrelation of the squared residuals, a good fitness would generate squared residuals equal to a white noise process, yet the limitation of this approach is basically the autocorrelation of a series is calculated through a graphical analysis of the autocorrelation and partial autocorrelation plots [reference the blogg of that cara].\n",
    "\n",
    "The figure (figure) shows how the plots of a poorly fitted model would look like. On the other hand, the figure (vdfds) shows the opposite scenario.\n",
    "\n",
    "![poorly fitted model](./resources/img/not_fitted_model.png \"poorly fitted model\")\n",
    "\n",
    "![well fitted model](./resources/img/fitted_model.png \"well fitted model\")\n",
    "\n",
    "As sawn in the figure (figure non fitted) for the well fitted model scenario, its AC and PAC plots show no autocorrelation or partial autocorrelation in the squered residuals outcomes. \n",
    "\n",
    "![white noise ac and pac plots](./resources/img/white_noise_ac_pac.png \"whte noise ac and pac plots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Combinatory Optimization problems and computational complexity \n",
    "\n",
    "One of the main challenges within combinatory optimization and searching problems is finding for the best solution (or global maximum/minimum depending of the problem to solve) among the set of the possible ones. In most cases, the mentioned set (or hyperspace) is composed by a countless number of possible solutions where reaching at the best one (or at least a suitable solution) requires certain intelligence or methodology rather than go through all the hyperspace which is quite more computationally-intensive.\n",
    "\n",
    "![Hyperspace](./resources/img/sol_hyperspace.png \"hyperspace\")\n",
    "\n",
    "These type of combinatory problems often are solved by using computational algorithms and they usually fit inside the **non-deterministic polynomial** problems (NP) family mainly because of its high complexity in execution time.\n",
    "\n",
    "Thus a problem could be fitted within the following categories regarding *computational complexity in execution time*:\n",
    "\n",
    "* **P type (polynomial):** Problems are solved by algorithms that deliver a solution in polynomial time (the ammount of time is possible to describe by a polynomial).\n",
    "\n",
    "* **NP type (non-deterministic polynomial):** Problems are solved by algorithms based on enumeration, nevertheless it is not possible to solve them in a reasonable time if the number of inputs is big.\n",
    "\n",
    "* **NP-Complete:** Strong evidence of the non existance of an algorithim whose *execution time* is a polynomial function depending of the input size.\n",
    "\n",
    "* ** NP-Hard: ** Optimization problems whose \"decision problems\" are *NP-Complete*.\n",
    "\n",
    "\n",
    "![Problems regarding *computational complexity in execution time*](./resources/img/complexity_time.png \"Problems regarding *computational complexity in execution time*\")\n",
    "\n",
    "\n",
    "Algorithms and techiques based on specific phenomena and heuristics  were devolped by some computer scientist gurus in order to deal with the *Acceptable execution time* and *a good solution* tradeoff when a problem belongs to one of the categories mentioned above. Between those techiques is possible to find algorithims based on entropy such as Simulated annhealing, evolutionary algorthims, multi agents-based, among others.\n",
    "\n",
    "..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Genetic Algorithms\n",
    "\n",
    "Proposed by the mathematician and physiscs John Henry Holland, Ph.D. Genetic algorthms are based on the genetic processes that take place in organisms according to the principle of natural selection and evolution dictated by Darwin (1859). They work with a population of individuals where each one represents a possible solution to a given problem.  \n",
    "\n",
    "The GA power comes from the fact it is a robust technique capable of dealing successfully with a broad spectrum of problems in diverse areas, including the ones which find limitations when working with other techiques. While the GA don't gauarantee to reach for a global maximum, empirical evidence exists suggesting they usually find acceptable solutions in a very competitive time against other combinatory optimization algorithms[GA reference], nevertheless, for the type of problems where specializaed techiques for their resolution is known or exists, GA is not the best option, although in these mentioned scenarios hybrid approaches (specialized known techique plus GA) are more than suitable offering significant improvents regarding execution time and fitness degree.\n",
    "\n",
    "\n",
    "![GA workflow*](./resources/img/GA_workflow.png \"Problems regarding *GA workflow*\")\n",
    "\n",
    "\n",
    "### 2.7.1. Codification\n",
    "\n",
    "It is assumed each indidivual could be represented as a set of parameters (denominated genes), when they are grouped it is formed a string of values often referred as *cromosomes*. While the used alphabet for representing an individual should not neccesarely br composed by {0,1}, most part of the theory which GA algorithms stand on, uses the mentioned alphabet.\n",
    "\n",
    "In biological terms, the set of parameters representing a *cromosome* is denominated *phenotype*. The *phenotype* contains the required information for build an organism (referred as genotype). The fitness of an individual to a certain problem depends of the genotype evaluation. \n",
    "\n",
    "This evaluation refers to a given score based on the fitness an indivual reaches to a certain *objective function* (e.x. In the nature is likely the hability to compete for resouces). The nature of this function depends on the specific problem to be maximized/minimized. \n",
    "\n",
    "During the *reproduction* phase, individuals are choosen for crossing themselves with each other and thus create the descendents or the individuals of the next generation. The parent selection then, is a probabilistic process described as a *selection process function* proportional to the *objective function* [ref tesis] and favoring the ones with the greater fitness to the problem. This process is based on the *skewed roulette* scheme which indicates the well fitted indivduals will be probably selected several times per generation, while the poorly fitted indivduals will be selected ocasionally.\n",
    "\n",
    "Once the a couple of parents is selected, their cromosomes are crossed with each others usually using *crossover* and *mutation* operators.\n",
    "\n",
    "The *crossover* operator, takes the parents strings and randomly cuts them in a specific point in order to produce a couple of initial (head) and a couple of final (tail) substrings. Subsequently the final substrings or tails are switched producing two new cromosomes. \n",
    "\n",
    "\n",
    "![Crossover operator based in a single point](./resources/img/crossover_one_point.png \"Crossover operator based in a single point\")\n",
    "\n",
    "This operator is known as *single point based crossover operator*, and frequenlty the decisition whether it is applied to a couple of parents or not, relies on a random process with a problility between 0.5 and 1.0 of being applied. In the case of the couples that weren't crossed by this process, their descendents are obtained just by duplicating them.\n",
    "\n",
    "The *mutation operator* is applied to each children individually. It consists in randomly alterating (normally with a low probability) each gen of the cromosome (*gaussian mutation* [deap ref.] ). \n",
    "\n",
    "![https://localwire.pl/graphinder-genetic-algorithm-mutation/](./resources/img/mutation.png \"Mutation operator\")\n",
    "\n",
    "Either both operators allow the algorithm to reach a balance between exploration-exploiting *tradeoff*. While the *convergence op.* advocates for maximizing the exploration of the hyperspace in higher but more superficial degree, the *mutation op.* guarantees a more detailed exploration over the hyperspce (exploiting). The later one is also the main responsible of the convergence degree of the algorithm.\n",
    "\n",
    "In practical terms the *convergence* definition given by De Jong (1975) in his doctoral thesis, turns out very useful: If the GA has been implemented correctly, the population will evolve increasing either the fitness of its best individual and the average fitness of all population towards the *optimal global* generation by generation.\n",
    "\n",
    "\n",
    "![convergence](./resources/img/convergence.png \"convergence\")\n",
    "\n",
    "This concept is related with the progression towards uniformity: A gen has converged when at least 95% of the individuals shared the same value for such gen. It is told a populations converges when all genes have converged but this definition can be generalized to a $\\beta \\%$ of converged individuals. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.1. Asset selection criteria\n",
    "\n",
    "Cryotocurrencie criteria: \n",
    "1. The cryptocurrencies with the higher value \n",
    "2. Cryptocurriencies with a price hstory track larger than a year and a half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Gathering data\n",
    "\n",
    "Collecting the data of the stock prices often implies a considerable monetary cost. The Sao Paulo stock market B3 for instance, offers this as a service where the interested brokers pay for obtaining the updated state of the prices as fast as possible. Such data however, is realised and made public after 15 minutes of its generation which means the brokers partially pay for those 15 minutes of difference. The other intrinsic  services the brokers pay for are the  transmission channels the data is delivered in. Usually these type of solutions are customized and robust as they maximize reliability, availability and scalability. Exposed as APIs.\n",
    "\n",
    "In the case these services are not affordable, the challenge relies on devoloping adequate software tools for colecting the required data, when it is public. Considering either in stock and cryptocurrency market, this information is open through several financial websites such as Google Finance (Stocks), Yahoo finance (Stocks), Bloomberg (Stocks), Coinmarketcap (cryptocurrencies), etc. The main challenge stands in decoding and parsing it from pure HTML to the desired format.\n",
    "\n",
    "The approach used in this work is the same as the mentioned above, it relies on executing a sequence of HTTP request and then parsing the HTML code obatined from downloading the entire website using Python libraries such as *urllib3*, *beatifusoup* and *pandas*.\n",
    "\n",
    "### 3.2.1. Web Datasorces:\n",
    "\n",
    "The main criterion for choosing the most adequate web datasource was the ease at automating the sequence of HTTP request and the consistency of the HTML components containing the relevant information.\n",
    "\n",
    "\n",
    "#### 3.2.1.1. **Yahoo finance: **  \n",
    "Among the anlysed web sources, mainly for brazilian stocks prices, the one that was easier to parse was *Yahoo Finance*. As specified earlier, the HTML was parsed in order to extract the relevant information.\n",
    "\n",
    "![yahoo web template](./resources/img/image_yahoo_template.png \"yahoo web template\")\n",
    "\n",
    "\n",
    "    * HTTP request: Each request (GET wikipedia) represents a stock historical price track,\n",
    "\n",
    "\n",
    "https://br.financas.yahoo.com/quote/{asset_symbol}/history?period1={initial_date}&period2={end_date}&interval=1d&filter=history&frequency=1d\n",
    "\n",
    "\n",
    "        * Paramters:\n",
    "            * asset_symbol: The symbol that identifies a certain company in the market (e.x. \"VIVT4.SA\").\n",
    "            * initial_date: Defines the starting point at which the considered time interval of the historical data is defined. \n",
    "            * end_date: Defines the ending point at which the considered time interval of the historical data is defined.\n",
    "        \n",
    "        In this case, either *initial_date* and *end_date* parameters require a long representation of the date or the ammount of seconds passed since 01-01-1970 until the specified date.\n",
    "\n",
    "Yahoo website also presented certain limitations when was intended to download an extense stock historical track. It was perceived that intervals above 150 days (e.x. init_date = '01-01-2018' and end_date = '01-05-2018') do not work correctly as is only displayed part of the information. Therefore the download process was splitted in chunks of 90 days-size per stock.\n",
    "\n",
    "![download flow](./resources/img/download_flux.png \"download flow\")\n",
    "\n",
    "Each iteration outcome is a 90 days-sized pandas dataframe, where later are merged together (*union*) in order to consolidate the history track as one.\n",
    "\n",
    "![Stock Pandas Dataframe](./resources/img/stock_pandas.png \"Stock Pandas Dataframe\")\n",
    "\n",
    "As sawn in the yahoo web, the date format within the table contains the dates naming the months with portuguese prefixes (e.x. *03 de ago de 2018*), hence a work was done by re-mapping such format to a more generic one (the sawn at the pandas dataframe image) aiming to facilitate the merge process to come (detailed later).\n",
    "\n",
    "#### 3.2.1.2. Coinmarketcap: \n",
    "\n",
    "Similarly to Yahoo finance, Coinmarketcap implements GET requests, furthemore issues as the detected in the Yahoo website, were not found in it.\n",
    "\n",
    "![Coinmarketcap template](./resources/img/coinmarket_cap_template.png \"Coinmarketcap template\")\n",
    "\n",
    "* HTTP request: Each request (GET wikipedia) represents a cryptocurrencie historical price track,\n",
    "\n",
    "\n",
    "https://coinmarketcap.com/currencies/{crypto_name}/historical-data/?start=20170101&end=20180731\n",
    "\n",
    "\n",
    "        * Paramters:\n",
    "            * crypto_name: The name of the cryptocurrencie (e.x. \"Ethereum\").\n",
    "            * start: Defines the starting point at which the considered time interval of the historical data is defined. \n",
    "            * end: Defines the ending point at which the considered time interval of the historical data is defined.\n",
    "        \n",
    "Different of Yahoo, in this case either *start* and *end* are represented by a more legible date format: *YearMonthDay* (e.x. 2018-08-31). Aditionally there was not need of splitting the download flow in chunks, coinmarketcap is capable to display all the data without implicit additional restrictions besides the date filters.\n",
    "\n",
    "![Cryptocurrencies download flow](./resources/img/crypto_download_flow.png \"Cryptocurrencies download flow\")\n",
    "\n",
    "Hence, the outcome dataframe structure shares the same format the stock pandas dataframe has, considering a similar work regarding re-mapping the date format in coinmarketcap  was also implemented.\n",
    "\n",
    "![Crypto Pandas Dataframe](./resources/img/crypto_pandas.png \"Crypto Pandas Dataframe\")\n",
    "\n",
    "\n",
    "### 3.2.2. Merging the data:\n",
    "\n",
    "By considereing the date column as the index of both dataframe, a simply join between both of them generated was done.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Time series pre-processing\n",
    "\n",
    "Considering the expression (number), by logarithm propierties we have\n",
    "\n",
    "$$r_t = ln(P_t) - ln(P_{t-1})$$\n",
    "\n",
    "As it was previously defined, $r_t$ or *return rate* represents therefore, a first order derivate of the logarithms of the prices. Usually this is applied in order to avoid biases caused by the non-stationary in trend and variance as they are difficult to model.\n",
    "\n",
    "Recapitulating, a *stationary process* is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.\n",
    "\n",
    "![stationarity comparison](./resources/img/stationary_comparison.png \"stationarity comparison\")\n",
    "\n",
    "In order to check whether a time series is stationary or not, Augmented Dickeyâ€“Fuller test is applied. The augmented Dickeyâ€“Fuller (ADF) statistic, used in the test, is a negative number the more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence [1 wikipedia ref]. Hence, the tresholds or critical values for rejecting the null hypothsis are given by the table below [2 ref]. \n",
    "\n",
    "![dickey_fuller critical values for t-student ditribution](./resources/img/df_critical_values_tstudent.png \"dickey_fuller critical values for t-student ditribution\")\n",
    "\n",
    "Once the $r_t$ is calculated from each closing prices series, considering a t-student distribution, the ADF is calculated to check if the such series are stationary.  \n",
    "\n",
    "![Augmented dickey_fuller outcomes](./resources/img/ADF_outcome.png \"Augmented dickey_fuller outcomes\")\n",
    "\n",
    "According to the table (num) above, is sawn none of the ADF statistic of each $r_t$ series, is above the critical value with 5% of significance (series with less than 250 and more than 200 samples), therefore the null hypthesis is rejected for all of them, in other words all series are stationary.   \n",
    "\n",
    "By transforming the asset's time series in such a way, the GARCH modeling process is improved due the shrinking in the optimization grid space (or the number of the possbile combinations to test), as is going to be explained in sections to come."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4. Pre-processing : Nan values\n",
    "\n",
    "The NaN values prescence in the downloaded data was mainly due unexpcted issues such the lack of the complete history track (from 01-01-2017 until 31-07-2018) of some cryptocurrencues, and the lack of data regarding weekends and holydays for stock data.\n",
    "\n",
    "The NaN vallues were filled as '0' after applying the first transformation of the closing price series. Considering the nature of the first order derivated series, a '0' value means there is no any change inthe asset state at $t$ and at $ t-1 $. Is the same as filling the missing price at $t$ with the $t-1$ value in the original closing price series.\n",
    "\n",
    "Such strategy avoids to increase or decrease the value of the mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Simulation and Optimization workflow\n",
    "\n",
    "For all the conducted simulations within this work, three different *Dataframes* or portfolio types were taken to work with: (1) A porfolio composed only by stocks, (2) a portfolio composed only by cryptocurrencies and (3) a portfolio composed by the two of these asset types. The objective behind such categegorization is offering a benchmark to clearly acknowladge which type of portfolio would perform better under certain time states.\n",
    "\n",
    "Consequently, the optimization process main goal relies on finding the best possible configuration of how the resources should be distributed in the portfolio. The distribution then, is a set of percent values named \"*weights*\" assigned to each asset, for instance, considering a set of stocks  $\\{A,B,C\\}$, the distribution's $\\{ 40\\%, 30\\%, 30\\% \\}$ return, is very probable to be different than the distribution's $\\{ 10\\%, 40\\%, 50\\% \\}$ return. Hence is possible to describe the optimization process as an iterative process that tests several distributions and selects the ones with higher returns and lower variances. \n",
    "\n",
    "Computationaly, as the portafolio is represented by a Dataframe whose columns and rows correspond to asset's symbols and asset's return rates respectevally **(ref figs. get data)**, a programation based on *loops* was implemented in order to test each possible *weights* combination. Consequently each iteration implements a workflow containing all the steps in order to calculate the required outputs: Portfolio overall's return and risk (standard deviation).\n",
    "\n",
    "### 3.5.1. Single portfolio Iteration workflow\n",
    "\n",
    "#### 3.5.1.1 Predicting returns of the asset\n",
    "\n",
    "The first step is predicting the asset's return individually, for this, the adopted approach is fitting a GARCH model to each of the 30 last days asset's time series, in order to obtain a more reliable (at least in theory) prediction.\n",
    "\n",
    "Considering the concepts introduced in the section (reference the section of GARCH), each series then, is submitted to the implemented GARCH framework based on the optimization problem that seeks to reach out for the best *p, o ,q* parameters combination by minimizing the AIC value.   \n",
    "\n",
    "![GARCH framework output](./resources/img/garch_output.png \"GARCH framework output\")\n",
    "\n",
    "In such a way, firstly a GARCH model using *arch* library in Python is searched and fitted to the input series. Subsenqurntly an output containing all the statistics is generated as indicated in the figure (fig) with which is possible to compare with other obtained models from other corresponding *(p, o, q)* combinations and then pick the one with the lower AIC. A more detailed description of the framework process is given by the *pseudocode* below.\n",
    "\n",
    "![GARCH framework pseudocode](./resources/img/garch_framew_pseudo.png \"GARCH framework pseudocode\")\n",
    "\n",
    "\n",
    "Recalling what has been done during the *Time series preprocessing stage* (section) and as it was demostrated all the considered preprocessed series are stationary (fig plot ADickeytest), the enhacement obtained by delimiting the *o* parameter at 0 only proved to be helpful in execution time by reducing the spectrum of combinations to test, furthermore the set of *p* and *q* combinations is also required to be relatively small when working with a significant ammount of assets, hence, the selected range to work with regarding the combinations set restrictions, was $[1, 3]$ for either *p* and *q* values. \n",
    "\n",
    "For few series though, the implemented GARCH framework had *convergence issues*, in other words it was not capable of fitting out any acceptable model due the serie did not show any significant autocorrelated terms (either *autoregresive* or *moving average*). \n",
    "\n",
    "As working with an underfitted model is certainly equivalent to work with a random process for prediction, some literature recommends to work with an expected value instead [dell course reference], hence this principle is also applied when non-convergence in modeling occurs where in such cases the mean of the series is taken as the predicted value.\n",
    "\n",
    "\n",
    "\n",
    "#### 3.5.1.2. Generating random distribution of weights: \n",
    "\n",
    "Weights are usually generated by using computer random processes with a normal or t-student distribution respecting the restriction that the summatory of all weights must equal 1. The later is achivied by normalizing the obtained random weights as is presented in the expression (below give a number). \n",
    "\n",
    "$$\\frac{w_i}{\\sum_{j}w_j}$$\n",
    "\n",
    "\n",
    "\n",
    "#### 3.5.1.3. Calculate the overall return\n",
    "\n",
    "Considering the outputs generated by stages 1 and 2 of the workflow iteration, with all the single assets returns  and the weights calculated, is possible to determine the overall portfolio return (or expected return) based on the expression (EXPECTED RETURN NUMVER ECUATION).\n",
    "\n",
    "\n",
    "#### 3.5.1.4. Calculate the overall standard deviation (risk)\n",
    "\n",
    "Equaly to the mentioned in the section above, by taking the two first stages outputs (section xx and xx respectevely) it is calculated the overall portfolio standard deviation or risk according to the stablished by MPT in section (section)  \n",
    "\n",
    "\n",
    "### 3.5.2. Analysing all the simulated portfolios\n",
    "\n",
    "\n",
    "Each single iteration generates a simulated portfolio with random weights and with an equivalent overall return and risk. The number of executed iterations is defined parametrically working with 1000 iterations for this case, what would compose a single *simulation*. Also, recalling what has been said at the beginning of this section (number *Simulation and Optimization workflow*), a simulation is performed for each portolio category (*(1) Stock only, (2) Crypto only, (3) Mixed*) obtaining clustered look-like outputs (look at the figure plot).\n",
    "\n",
    "![simulation outcomes](./resources/img/simu_output.png \"Simulation outcomes\")\n",
    "\n",
    "#### 3.5.2.1. Drawing markowitz efficient frontier\n",
    "\n",
    "For this stage, the used method to calculate the Markowitz efficient frontier is described by first locating the risk interval formed by the standard deviations of the *lowest risk* and *higher return* portfolios (based on the two dimensional plane shown in figure (figure above)), secondly, such interval is splitted equally in order to determine the distance $D$ between the *reference points* over the plane (Expression below): The objective then is to reach for the maximum return at each of these points. Consquently the number of portfolios the *markowitz efficient frotiner* is composed by, is equal or less than the number of *reference points* or *risk levels*. \n",
    "\n",
    "\n",
    "$$D = \\frac{\\sigma_{higher \\ return \\ portfolio} - \\sigma_{lower \\ risk \\ portfolio}}{N}$$\n",
    "\n",
    "Where $N$ is defined parametricaly and corresponds to the number of the desired *reference points*. For this work such number is setted at 5.\n",
    "\n",
    "Finally, the resulting interval follows the sequence described on expression (exp below).\n",
    "\n",
    "$$Risk \\ Levels = [D,\\ 2D,\\ ...,\\ ND]$$ \n",
    "\n",
    "This approach however, presented some issues regarding the low probability at finding a portfolio located in a specified *risk level*, furthermore, even when a portafolio exists given a certain *risk level*, a broad part of the plane remains unexplored what does not guarantee the most optimal portfolios are considered. In order to overcome such limitations, an step forward was taken and instead of checking the existance of portfolios at a single point, the searching was changed by looking for a maximum between the spaces cutted by the already defined *risk levels* as shown in the figure (figure below).\n",
    "\n",
    "(Hacer grÃ¡fico)\n",
    "\n",
    "Finally in this stage, a Markowitz efficient frontier is calculated to each of the portfolio categories (Fig. below) \n",
    "\n",
    "![\"Markowitz efficient frontier\"](./resources/img/m_frontier.png)\n",
    "\n",
    "\n",
    "#### 3.5.2.2. Obtaining the portfolio with the best ratio of sharpe\n",
    "\n",
    "Considering the given expression (num ecuation section), a *Ratio of Sharpe* is calculated for each of the output frontiers given the portfolio categories. For such calculation the selected free-risk asset, required to measure the *opportunity cost* an investor might miss (section theory ratio of sharpe), was a Brazilian treasury bond asset with an annual return of 4% above the accumulated inflation.\n",
    "\n",
    "Such kind of fixed income investment products, are denominated as *indexed products* which deliver a fixed annual return above a certain economic index. In the Brazilian state, the most popular used index for this type of assets are IPCA (*Ãndice de PreÃ§os ao Consumidor*), CDI (*Certificado de DepÃ³sito InterbancÃ¡rio*) and SELIC (*Sistema Especial de LiquidaÃ§Ã£o e CustÃ³dia*) (ref).\n",
    "\n",
    "![list of assets to buy](./resources/img/products_fixed.png \"list of assets to buy\")\n",
    "\n",
    "As the time-windows stablished for evaluating the portfolios are given in days, the free-risk annual rate return was transformated to daily rate return based on the expression (expression above).  \n",
    "\n",
    "$$ Daily \\ Rate = (1 + Annual \\ Rate)^{\\frac{1}{360}} - 1 $$\n",
    "\n",
    "\n",
    "Besides the already pointed avdantages this metric represents to the investors in selecting the \"best suited\" portfalio when they are not able to decide at wich specific *risk level* the desire to operate, further benefits were considered for the sake of this work. Basically, having a single point (per portfolio category) rather than a set of them, may allow to analyse how the proposed optimization model would perform temporaly. (make image)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3. Genetic algorithms optimization workflow\n",
    "\n",
    "The GA for this proposal of a model are suitable mainly for the weighteting generation process: Until this point such stage was managed by a random number generator ( section 1.2), but as indicated in this work, a combinatory optimization problem (or NP problem in execution time) is better solved by algorithms of this type (section GA).\n",
    "\n",
    "Taking into account the theory regarding GA (section GA), in portfolio optimization problems then, a portfolio is the equivalent to an *indivudal* (or *cromosome*) where each of its genes correspond to a single weight *w* with values between [0,1] (fig. below - arregladas); from there, the weighting process is done by evaluating an individual through an *Objective Function* rather than gerate it randomly, which gives further capabilities in explorating and exploiting the solutions plane (fig. below).\n",
    "\n",
    "![\"Random-based weighting\"](./resources/img/ga_sim.png \"Random-based weighting\")\n",
    "![\"GA-based optimization\"](./resources/img/sim_ga.png \"GA-based optimization\")\n",
    "\n",
    "It is assumed the lower sillhoutte of the clusters, the higher the plane exploration degree (recalling a cluster represents a portfolio type (section above)).\n",
    "\n",
    "In order to assemble GA within the defined workflow (section), two things are worth being considered: *Define an adequate objective function* and *define the hyperparameters* of the GA implementation.\n",
    "\n",
    "#### 3.5.3.1. Objective function (O.F):\n",
    "\n",
    "The desired O.F must aim to maximize the *Expected Return* and *Risk* trade-off which could be expressed in several ways.\n",
    "\n",
    "(1) $$O.F = E(R_p) - \\sigma_{p}$$\n",
    "\n",
    "(2) $$O.F = \\frac{E(R_p)}{sigma_{p}}$$\n",
    "\n",
    "(3) $$O.F = \\sigma_{p} - \\frac{1}{E(R_p)}$$\n",
    "\n",
    "\n",
    "\n",
    "Among the presented options, certain problems were perceived in not linear O.F's usage: Even with the cryptocurrencies highier volatility compared with the stocks variance, such type O.F. trended to desconsider such difference effects, for instance: There wouldn't exist any difference between a cryptocurrencie with an expected return of 22% and a standard deviation of 11%,  and a stock with expected return of 0,8% and a standard deviation of 0,4%. Hence, aiming towards avoiding such kind of bias, a linear O.F was preferred (option (1)) over the other ones.\n",
    "\n",
    "\n",
    "#### 3.5.3.1. GA Hyperparameters\n",
    "\n",
    "| Hyperparameter  | Setted value  |\n",
    "| -------------   |:-----:|\n",
    "| Crossover       | *single point based crossover operator*  | \n",
    "| Mutation        | *Flip bit with a probability of **0,3** of the wieght being mutated* \n",
    "| Select          | *Roulette Strategy* (ref. take image wikipedia)|\n",
    "\n",
    "\n",
    "\n",
    "For such a probem, when mutating using *gaussian method* means a certain weight might be changed by any value outside the given range [0,1] because the O.F. implementation defines such restriction within itself and the mutation is a sequential yet an independent stage (fig below), which leads to inconsistent output values.\n",
    "\n",
    "![gaussian mutation example](./resources/img/gausian_mutation.png \"gaussian mutation example\")\n",
    "\n",
    "Thus, *Flip bit* mutation startegy is using instead, which consists only in taking the chosen genome and inverts the bits (i.e. if the genome bit is 1, it is changed to 0 and vice versa), which does no interefer with the iniatially setted weight within the O.F.  \n",
    "\n",
    "The other hyperparameter values presented in the table (num table) were setted that way due the easiness given by the needless to define explicity any numerical hardcoded parameter. However testing further configurations such as *touranemnt* strategy for selection, *flipping* for mutation and so on; is encouraged to be a future work.\n",
    "\n",
    "#### 3.5.3.2. Further GA implementation details\n",
    "\n",
    "The Python library used for the GA implemention is *Deap*: It offers a reliable framework regarding the code pattern it must be followed including a very rich documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Results\n",
    "\n",
    "\n",
    "This section intents to present the obtained results from conducting several optimization scenarios mainly focused towards analysing the optimal portfolios in different instants of time considering what has been exposed in section 3.5.2.2 about the main purpose of calculating the Ratio of Sharpe in this work.\n",
    "\n",
    "Each of those scenarios were applied separately for each of the portfolios categories defined in section 3.5. Taking into account the given restriction earlier regarding the 15 days window-time.  \n",
    "\n",
    "\n",
    "## 4.1. Portfolio categories outputs comparison\n",
    "\n",
    "The current scenario consists on executing the optimization model during 30 days daily.\n",
    "\n",
    "### 4.1.1. Returns outputs by portfolio category\n",
    "\n",
    "![Returns by day](./resources/img/returns_month.png \"Returns by day\")\n",
    "\n",
    "### 4.1.2. Risk outputs by portfolio category\n",
    "\n",
    "![Risk by day](./resources/img/risk_month.png \"Risk by day\")\n",
    "\n",
    "By watching the graph, it is sighly sawn at least, the optimization model trends to minimize the risk sometimes achiviing even lower risk values than the stock-only portolios and higher returns than both categories: Which certainly demosntrates what the Modern Portfolio Theory postulates about the benefits of diversificating.\n",
    "\n",
    "\n",
    "### 4.1.2. Error Analysis\n",
    "\n",
    "The standard error is defined by the expression (below). \n",
    "\n",
    "$$ Standard \\ Error = \\sqrt{\\frac{\\sum_{T}{{(\\hat{ER_t} - ER_t)}^2}}{n-2}} $$\n",
    "\n",
    "Where $\\hat{ER_t}$ is the predicted *Expected Return* (or the model output), and ${ER_t}$ is the real return calculated by taking the optimal portfolio weights and multiply them by the real returns in $t$.\n",
    "\n",
    "\n",
    "![Stock Portfolio Error](./resources/img/stock_error_month.png \"Stock Portfolio Error\")\n",
    "\n",
    "![Cryptocurencies Portfolio Error](./resources/img/crypto_error_month.png \"Cryptocurencies Portfolio Error\")\n",
    "\n",
    "![Mixed Portfolio Error](./resources/img/mixed_error_month.png \"Mixed Portfolio Error\")\n",
    "\n",
    "By calculating the error at specific time instants, the findings resulted in unexpected behaviours along with a poorly fitness between the real expected return and the predicted one, even when diversified portfolios presented the lowest stadard errors as the standard error of the estimation, is directly related with the category portfolio standard deviation or risk.\n",
    "\n",
    "\n",
    "\n",
    "### 4.1.3. Total\n",
    "\n",
    "For the investors get a meaningful insights of what the errors mentioned in section 4.1.2 represent, a comparison between the cumulative predicted return and the cumulative real return was implemented.\n",
    "\n",
    "An cumulative return refers to the total gain or lost in percent, that is obtained at the end of the period the proposed model ran (in this case, after 30 days).\n",
    "\n",
    "|Category| Real Cumulative Return  | Predicted Cumulative Return  |\n",
    "| :---------: | :-------------: |:-----:|\n",
    "| Stocks only | -3,84% | 23,19%  | \n",
    "| Cryptocurrencies only | -48,18% | -7,98% | \n",
    "| Mixed | -13.6823 | 15,5% |\n",
    "\n",
    "The cumulative error beneath the model, suggests is not affordable investing for long periods using this approach.\n",
    "\n",
    "\n",
    "### 4.1.4. Mixed Portfolio distribution\n",
    "\n",
    "In mixed portfolios, by watching the fig. (below) and comparing it with the fig. (risk), specificaly between 08-08-2018 and 14-08-2018, is perceived the optimization model minimizes the risk by allocating more resources in stocks rather than in crpyptocurrencies. \n",
    "\n",
    "![Daily portfolios resources distribution](./resources/img/montlhy_distr.png \"Daily portfolios resources distribution\")\n",
    "\n",
    "### 4.1.5. Optimal portfolios Real Returns\n",
    "\n",
    "There are not significant evidences then, a certain portfolio category would behave better than the others as the returns don't follow any pattern during the last month.\n",
    "\n",
    "![Optimal portfolios real returns](./resources/img/real_returns_month.png \"Optimal portfolios real returns\")\n",
    "\n",
    "\n",
    "## 4.2. The model running  yearly\n",
    "\n",
    "\n",
    "The current scenario consists on executing the optimization model during a year monthly.\n",
    "\n",
    "### 4.2.1. Returns outputs by portfolio category\n",
    "\n",
    "![Returns by month](./resources/img/returns_year.png \"Returns by month\")\n",
    "\n",
    "### 4.2.2. Risk outputs by portfolio category\n",
    "\n",
    "![Risk by month](./resources/img/risk_year.png \"Risk by month\")\n",
    "\n",
    "### 4.2.3. Error Analysis\n",
    "\n",
    "Using the expression (num ec 4.1.3.),\n",
    "\n",
    "![Stock Portfolio Error](./resources/img/stock_error_year.png \"Stock Portfolio Error\")\n",
    "\n",
    "![Cryptocurencies Portfolio Error](./resources/img/crypto_error_year.png \"Cryptocurencies Portfolio Error\")\n",
    "\n",
    "![Mixed Portfolio Error](./resources/img/mix_error_year.png \"Mixed Portfolio Error\")\n",
    "\n",
    "### 4.2.4. Portfolio distribution\n",
    "\n",
    "The distribution certainly evolves depending market time of an asset type. i.e.  Most part of the resources were allocated in cryptocurrencies while they got an up-trending.\n",
    "\n",
    "![Montlhy portfolios resources distribution](./resources/img/distr_year.png \"Montlhy portfolios resources distribution\")\n",
    "\n",
    "\n",
    "### 4.2.5. Optimal portfolios Real Returns\n",
    "\n",
    "\n",
    "Same monthy experiment\n",
    "\n",
    "![Optimal portfolios real returns](./resources/img/real_return_year.png \"Optimal portfolios real returns\")\n",
    "\n",
    "\n",
    "## 4.3. Mean only Vs. GARCH usage\n",
    "\n",
    "Although in the methology section (specificaly in 3.5.1.1. part) was indicated an algorithm was implemented to fit each asset series to a GARCH model in order to predict the asset rate return, further tests were conducted to validate whether its usage meant any improvement. Sadly, it was perceived there was not the case and the enhacement by using GARCH models, was unexisting if compared with just using an Expected Value or Mean as the predicted asset return instead.\n",
    "\n",
    " ![garch errors mixed](./resources/img/garch_error_mixed.png \"GARCH errors\")\n",
    "\n",
    "Furthemore, in order to discard any exogenus effect regarding *portfolio categories* varible, it was not observed there is no different behaviour either for portfolios composed just by cryptocurrencies and portfolios just by stocks. \n",
    "\n",
    "\n",
    "\n",
    "In order to determine whether exists a significative \n",
    "\n",
    "table of simulations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4.4. Time windows, 15 days of history track, is the middle between a random prediction, copmaring weerror with 30%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The targeted degree of automation intended in this work, was counterproductive in issues such as the asset's return forecasting using GARCH models: The complexity in fully automating such procedure, established some critical restrictions that led to:\n",
    "    * The lack of enough Data due the short-sized time-windows able to consider.   \n",
    "    * Few combinations of *p*,*q* values to test, consequently, lower probability of reaching out a better model to fit. It is evidenced then, the craft component regarding the graphical analysis over the AC (autocorrelation) and PAC (partial autocorrelation), isn't recommended to left behind because of the ease it represents at detecting high order auto-regressive and moving-average correlated parameters, which otherwise would be a very expensive compuntaional task to perform.\n",
    "\n",
    "* Even by using MPT, obtaining money invensting in such volatile markets, turns out very challmeging. As sawn, the returns of  . This result may be improved by using other approvahes for forecasting...\n",
    "\n",
    "\n",
    "\n",
    "* Negative correlation between crypto=currencies and stock market assets.\n",
    "\n",
    "Some experts stand of the hyphothesis there is a weak negative correlation between criptocurrencies returns and stock market assets returns regarding FEAR index cotation, it means when there exists hihgher expeculation and fear in markets such as criptocurrencies, the subjective risk is transferred to the stock market.\n",
    "\n",
    "In order to appreciate such phenomena, a heatmap showing correlations between all the assets considered is calculated.\n",
    "\n",
    "![correlation between the considered assets](./resources/img/heatmap.png \"correlation between the considered assets\")\n",
    "\n",
    "An approximatelly behaviour of the one described in the hypthotesis above, is showed in the data as well, menaning it works also for Brazilian stock assets.\n",
    "\n",
    "This fact remarks the importance of diversfying this way according to the principles of diversfication proposed by the MPT: \"Is less risky to get differents types of products than just having one type\"\n",
    "\n",
    "\n",
    "* Assets selection\n",
    "    * Paretoâ€™s law, considering just most valuable companies operating in the market in order to avoid undesired effects like â€œtoroidâ€ and â€œâ€¦â€ (Check with Rodrigo the name of these effects).\n",
    "    * The same applied to Criptocurrencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
